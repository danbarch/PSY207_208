---
title: "Multiple Regression"
subtitle: "Part 4: Intro to Bayesian Regression"
author: ""
date: "`r Sys.Date()`"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [nighttime_in_the_big_city.css]
nature: 
  highlightStyle: tomorrow-night-blue
  hilightLines: true
---
  
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.showtext = TRUE)
library(tidyverse) # load tidyverse package
library(MASS)
library(ggplot2)
library(ggthemes)
library(kableExtra)
library(flextable)
library(cowplot)
library(xaringanthemer)
#library(gg3D)
library(leaflet)
library(maps)
library(fontawesome)
library(officer)
library(officedown)
library(flextable)
library(ftExtra)
library(gtsummary)
library(BAS)

# DF

set.seed(12)
# create the variance covariance matrix
sigma<-rbind(c(1,-0.6, 0.5), c(-0.6,1, -0.2), c(0.5, -0.2, 1))
# create the mean vector
mu<-c(7, 12, 5) 
# generate the multivariate normal distribution
df<-as.data.frame(mvrnorm(n=50, mu=mu, Sigma=sigma)) %>% 
  rename(y = V1,
         x1 = V2,
         x2 = V3) %>% 
  arrange(x1) %>% 
  mutate(x3 = rnorm(50, 8)) %>% 
  mutate(zy = (y - mean(y)/sd(y)),
         zx1 = (x1 - mean(x1)/sd(x1))) 

# Replicated DF

set.seed(34)
# create the variance covariance matrix
sigma<-rbind(c(1,-0.6, 0.5), c(-0.6,1, -0.2), c(0.5, -0.2, 1))
# create the mean vector
mu<-c(7, 12, 5) 
# generate the multivariate normal distribution
df_replication<-as.data.frame(mvrnorm(n=50, mu=mu, Sigma=sigma)) %>% 
  rename(y = V1,
         x1 = V2,
         x2 = V3) %>% 
  arrange(x1) %>% 
  mutate(x3 = rnorm(50, 8)) %>% 
  mutate(zy = (y - mean(y)/sd(y)),
         zx1 = (x1 - mean(x1)/sd(x1))) 
## To create pdf version:


# setwd("~/Documents/PSY 208/PSY_207-208_23-24/Lectures")
# renderthis::to_pdf("Bayesian_Regression.Rmd", complex_slides = TRUE)
```

### Bayesian Inference Review

.slightly-smaller[

In ***Markov Chain Monte Carlo Methods***, we made **Bayesian inferences** on the **probability** that a coin would land on *heads* based on some **observed data**:

]

| Heads | Tails |
|:-----:|:------|
| 48    |   52  |

***

.slightly-smaller[

.pull-left[

We started our analysis with **two stipulations**:

> A **flat prior**

> The **binomial likelihood function**

]

.pull-right[

And we used **two different methods**:

> The **Metropolis-Hastings Algorithm**

> Using **conjugate** functions for the **prior** and the **posterior** distributions

]
]

---

### Example: Prior Distribution

We started by assuming that **every possible value of $\pi$ is equally likely**.

.pull-left[

This results in a **uniform distribution**

> We will **sample from this distribution**

> The prior is reflected by sampling **any value** between 0 and 1 with **equal probability**.

]

.pull-right[

```{r echo = FALSE, fig.height = 6}
data.frame(x = seq(0, 1, 1/1000)) %>% 
  mutate(y = dbeta(x, 1, 1)) %>% 
  ggplot(aes(x, y))+
  geom_line(linewidth = 2)+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  theme(axis.ticks = element_blank(),
        plot.title = element_text(family = "sans",
                                  size = 30),
        plot.subtitle = element_text(family = "sans",
                                     size = 24))+
  labs(title = bquote("Beta distribution ("*alpha==1*","~beta==1*")"),
       subtitle = "also a uniform distribution (min = 0, max = 1)",
       )+
  scale_y_continuous(breaks = c(0.975, 1, 1.025))
```

]

---

### Example: Likelihood Function

The coin flips are **binomial**; the probability that we got 48 heads in 100 flips *given* some probability $\pi$ is given by the **binomial likelihood function**

$$p(D|H)=p(s|N, \pi)=\frac{N!}{s!f!}\pi^s(1-\pi)^f$$
We take the **sample $\pi$ values** we take from the ** $\text{Beta}(1, 1)$ distribution** and plug them in to the function to get **likelihood values for each potential $\pi$ value**

---

### The Metropolis-Hastings Algorithm

The **Metropolis-Hastings Algorithm**\* is a **Markov Chain Monte Carlo (MCMC)** method that generates **Bayesian Posterior Distributions**

Three things to keep in mind:

1. **Posterior probabilities** are the **outputs** of **Bayes's Theorem**.

2. A **posterior distribution** is a **representation of information about the probability of possible values of population parameters**

So, the distributions generated by the M-H algorithm are **determined by Bayes's Theorem**.


.footnote[

\*Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller (1953); Hastings (1970)

]


---
### The Metropolis-Hastings Algorithm

.slightly-smaller[

1. Choose a starting parameter value. That's the first **current** value.

2. Generate a **proposed** parameter value from the prior distribution

3. Calculate the ratio ** $r$** between the probability of the observed data given the **proposed** value to the probability of the observed data given the **current** value. 

4. If $r \ge 1$, accept the **proposed** parameter.

  > If $r > u(0, 1)$ (where $u$ represents the *uniform distribution*), accept the **proposed** parameter.
 
  > else, accept the **current** parameter.

5. Store the **accepted** parameter. That becomes the new **current parameter**. 

6. Repeat steps 1 - 5 as many times as you like (but a lot of times).

]

---

### The Metropolis-Hastings Algorithm

> We'll **sample** all of our parameter values from the **prior distribution** according to their **prior probabilities**; so ** $p(H)$** cancels in the numerator and denominator of the probability ratio ** $r$**.

> The base rate ** $p(D)$** is the same in the numerator and denominator, so **that cancels, too**.

Therefore, our main concern is the **likelihood function $p(D|H)$**

---

### The Normal Likelihood Function

.slightly-smaller[

The **normal density function** is the **likelihood** of observing ** $\color{#d2aadd}{x}$** **given** ** $\mu$** and ** $\sigma^2$**:

$$p(\color{#d2aadd}{x}|\color{#f5e042}{\mu}, \color{#f5e042}{\sigma^2})=\frac{1}{\sqrt{2\pi\color{#f5e042}{\sigma^2}}}e^{-\frac{(\color{#d2aadd}{x}-\color{#f5e042}{\mu})^2}{2\color{#f5e042}{\sigma^2}}}$$

The **normality assumption** for regression\* implies that the ** $\color{#d2aadd}{y_i}$** values -- *given* ** $\color{#d2aadd}{x_i}$**, the coefficients ** $\color{#f5e042}{\beta_0}$** and ** $\color{#f5e042}{\beta_1}$**, and the error variance ** $\color{#f5e042}{\sigma^2}$** -- are **normally distributed**


$$\color{#d2aadd}{y_i} \sim N\left(\color{#f5e042}{\beta_0+\beta_1}\color{#d2aadd}{x_i}, \color{#f5e042}{\sigma^2}\right)$$
The **prediction** is the $\mu$; the variance is the **error variance**. We're going to sub those into the **normal likeilihood function**

]


.footnote[

***

\*which we have shown is an assumption that multivariate observations are drawn from **multivariate normal distributions**
]

---

### Regression Application


The **likelihood** of **each** $\color{#d2aadd}{y_i}$** *given* the predictor variable ** $\color{#d2aadd}{x_i}$**, **regression coefficients ** $\beta_0$** and ** $\beta_1$**, and error variance ** $\sigma^2$** is:

$$f(\color{#d2aadd}{y_i}|\color{#d2aadd}{x_i}, \color{#f5e042}{\beta_0}, \color{#f5e042}{\beta_1},  \color{#f5e042}{\sigma^2})=\frac{1}{\sqrt{2\pi\color{#f5e042}{\sigma^2}}}e^{-\frac{1}{2}\left[\frac{(\color{#d2aadd}{y_i}-(\color{#f5e042}{\beta_0} + \color{#f5e042}{\beta_1}\color{#d2aadd}{x_i}))^2}{\color{#f5e042}{\sigma^2}}\right]}$$

.center[
(it's just the normal function with *prediction errors* instead of $x-\mu$)
]


---

### Regression Application


The **likelihood** of **all** the ** $\color{#d2aadd}{y}$** values (*given* ** $\color{#d2aadd}{x}$**, ** $\beta_0$** ** $\beta_1$**, and ** $\sigma^2$**) is the **product** of all the individual ** $\color{#d2aadd}{y_i}$** likelihoods:

$$\begin{aligned} p(\color{#d2aadd}{y}|\color{#d2aadd}{x_i}, \color{#f5e042}{\beta_0}, \color{#f5e042}{\beta_1},  \color{#f5e042}{\sigma^2}) &= p(\color{#d2aadd}{y_1} \cap \color{#d2aadd}{y_2} \cap \cdots \cap \color{#d2aadd}{y_n})=p(\color{#d2aadd}{y_1})p(\color{#d2aadd}{y_2})\cdots p(\color{#d2aadd}{y_n}) \\ &= \prod_{i = 1}^n p(\color{#d2aadd}{y_i}|\color{#d2aadd}{x_i}, \color{#f5e042}{\beta_0}, \color{#f5e042}{\beta_1},  \color{#f5e042}{\sigma^2}) \end{aligned}$$
***

So our **overall likelihood function** is:

$$f(\color{#d2aadd}{y_i}|\color{#d2aadd}{x_i}, \color{#f5e042}{\beta_0}, \color{#f5e042}{\beta_1},  \color{#f5e042}{\sigma^2})=\left(2\pi\color{#f5e042}{\sigma^2}\right)^{\frac{-n}{2}}e^{-\frac{1}{2\sigma^2}\left[\sum_{i=1}^n\color{#d2aadd}{y_i}-(\color{#f5e042}{\beta_0} + \color{#f5e042}{\beta_1}\color{#d2aadd}{x_i})^2\right]}$$

and that's the **likelihood** that we will use in the **Metropolis-Hastings Algorithm**
---
### Bayesian Regression with MCMC

For example, let's use the sample data in the scatterplot on the left; frequentist model estimates are on the right.

.left-column[

```{r echo = FALSE, fig.width = 3}
df %>% 
  ggplot(aes(x = x1, y = y))+
  geom_point(size = 3)+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  geom_smooth(method = "lm", se = FALSE)
```


]
.right-column[

.smaller-output[

```{r echo = FALSE}
summary(lm(y ~ x1, data = df))
```
]
]

---

### Bayesian Regression with MCMC

.slightly-smaller[

We are going to set up **prior distributions** for the variables ** $\beta_0$**, ** $\beta_1$**, and ** $\sigma^2$** that are **weakly informative**

> **weakly informative** priors use a *little* pre-analysis information

Our **prior distributions** for $\beta_0$ and for $\beta_1$ will be normal distributions

> ** $p(\beta_0) \sim N(13, 25)$**: we'll start with the frequentist estimate of the mean to be more efficient and give it a big old variance.

> ** $p(\beta_1) \sim N(0, 4)$**: 0 is a pretty typical starting point for a predictor; 4 is $4x$ the standard normal

> ** $p(\sigma) \sim \text{Gamma}(1, 1)$**: The **Gamma** distribution is a family that includes the ** $\chi^2$ distribution**. It has mathematical properties that lead many Bayesians to prefer it over the $\chi^2$ for variances $\text{Gamma}(1, 1)$ is the **standard Gamma**.

]

---

### Bayesian Regression with MCMC

Here's how we could set up the Metropolis-Hastings Algorithm to estimate a **Bayesian model**

> *i.e.*, generating **posterior distributions** on ** $\beta_0$**, ** $\beta_1$**, and ** $\sigma^2$**

First, we'll code up the **likelihood function**:

```{r}
# set a seed for reproducible results
set.seed(77)

likelihood <- function(y_i, x_i, sigma, beta_0, beta_1){
  n <- length(y_i) # n is the number of observations
  # the normal equation
  (2*pi*sigma^2)^(-n/2)*exp((-1/(2*sigma^2))*
                              sum((y_i - beta_0 - beta_1*x_i)^2))
}

```

---

### Bayesian Regression with MCMC

Next, we'll set up **vectors to hold our iterations for each variable**

> We'll put our initial guesses in the first spot of each vector

```{r}
b0vec <- c(13, rep(NA, 999999)) # intercept vector
b1vec <- c(0, rep(NA, 999999)) # coefficient vector
sigmavec <- c(2, rep(NA, 999999)) # sigma vector
```


---

### Bayesian Regression with MCMC

.slightly-smaller[Finally, here's the **Metropolis-Hastings Algorithm**]

```{r}
for (i in 2:1000000){ # 1 million iterations
  b0_current <- b0vec[i-1]
  b0_proposed <- rnorm(1, 13, 5) # prior normal
  b1_current <- b1vec[i-1] 
  b1_proposed <- rnorm(1, 0, 2)  # prior normal
  sigma_current <- sigmavec[i - 1]
  sigma_proposed <- sqrt(rgamma(1, 1, 1)) # prior gamma 
  u <- runif(1) # random number between 0 and 1
  # the likelihood ratio r
  r <- likelihood(df$y, df$x1, sigma_proposed, b0_proposed, b1_proposed)/
    likelihood(df$y, df$x1, sigma_current, b0_current, b1_current)
  # the acceptance-rejection algorithm
  b0vec[i] <- ifelse(r >= 1, b0_proposed,
                      ifelse(r > u, b0_proposed, b0_current))
  b1vec[i] <- ifelse(r >= 1, b1_proposed,
                      ifelse(r > u, b1_proposed, b1_current))
  sigmavec[i] <- ifelse(r >= 1, sigma_proposed,
                      ifelse(r > u, sigma_proposed, sigma_current))}
```

---

### Bayesian Regression with MCMC

```{r echo = FALSE}
mcmcdata <- data.frame(labels = c(rep("b0", length(b0vec)),
                      rep("b1", length(b1vec)),
                      rep("sigma", length(sigmavec))),
           iteration = rep(1:length(b0vec)),
           data = c(b0vec,
                    b1vec,
                    sigmavec^2)) 


b0hist <- mcmcdata %>% filter(labels == "b0") %>%  
  ggplot(aes(data))+
  geom_histogram(binwidth = 0.5,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  theme(axis.title.x  = element_text(family = "sans"))+
  labs(x = bquote(beta[0]))

b1hist <- mcmcdata %>% filter(labels == "b1") %>%  
  ggplot(aes(data))+
  geom_histogram(binwidth = 0.05,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  theme(axis.title.x  = element_text(family = "sans"))+
  labs(x = bquote(beta[1]))

sigmahist <- mcmcdata %>% filter(labels == "sigma") %>%  
  ggplot(aes(data))+
  geom_histogram(binwidth = 0.05,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
    theme(axis.title.x  = element_text(family = "sans"))+
  labs(x = bquote(sigma^2))


```

.pull-left[

These are the resulting **posterior distributions** on ** $\beta_0$**, ** $\beta_1$**, and ** $\sigma^2$**

]

.pull-right[

```{r echo = FALSE, fig.height = 4}
b0hist + xlim(5, 20)
```

]

.pull-left[

```{r echo = FALSE, fig.height = 4}

b1hist

```

]

.pull-right[

```{r echo = FALSE, fig.height = 4}

sigmahist+xlim(0.25, 2)

```

]
---
### Bayesian Regression with MCMC

```{r echo = FALSE}
b1ci <- quantile(b1vec, c(0.025, 0.975))
```

With Bayesian methods, we use the **posterior distributions** to make statements about **posterior probability**. 

> For example, let's look at the posterior distribution for** $\beta_1$**:

.pull-left[

The **posterior probability** that $`r round(b1ci[1], 2)` \le \beta_1 \le `r round(b1ci[2], 2)` = 95\%$

```{r echo = FALSE, fig.height = 4}

b1hist_ci <- mcmcdata %>% 
  filter(labels == "b1") %>%  
  ggplot(aes(data))+
  geom_histogram(binwidth = 0.05,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  theme(axis.title.x  = element_text(family = "sans"))+
  labs(x = bquote(beta[1]))+
  geom_segment(x = b1ci[1],
               xend = b1ci[2],
               y = Inf,
               yend = Inf,
               size = 5,
               color = "#f5e042")+
  annotate("text",
           x = b1ci[1],
           y = Inf,
           hjust = 1.2,
           vjust = 1,
           label = "95% HDI",
           size = 8,
               color = "#f5e042")
  

b1hist_ci
```

]

.pull-right[

The **posterior probability** that $\beta_1 > 0 =  `r round(100*sum(b1vec < 0)/length(b1vec), 2)`\%$

```{r echo = FALSE, fig.height = 4}

b1hist_le0 <- mcmcdata %>% 
  filter(labels == "b1") %>%  
  ggplot(aes(data))+
  geom_histogram(binwidth = 0.05,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  theme(axis.title.x  = element_text(family = "sans"))+
  labs(x = bquote(beta[1]))+
  geom_segment(x = 0,
               xend = 0,
               y = 0,
               yend = Inf,
               size = 5,
               color = "#f5e042",
               lty = 2)
  

b1hist_le0
```
]
---

### Bayesian Inference

.slightly-smaller[

Reminder: unlike frequentist inference, Bayesian inference is **explicitly probabilistic**

.pull-left[

```{r echo = FALSE, fig.height = 3, fig.width = 4}
set.seed(99)

null_t <- rt(150, df = 30)
t_le0 <- paste0(round(100*sum(null_t <= 0)/length(null_t), 2), "%")
t_g0 <- paste0(round(100*sum(null_t > 0)/length(null_t), 2), "%")

ggplot(data.frame(x = null_t), aes(x))+
  geom_histogram(binwidth = 0.25,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")+
  theme(axis.title.x  = element_text(family = "sans"))+
  labs(x = bquote(beta), y = "frequency")+
  geom_segment(x = 0,
               xend = 0,
               y = 0,
               yend = Inf,
               size = 1,
               color = "#f5e042")+
  annotate("text",
           x = -Inf,
           y = Inf,
           hjust = 0,
           vjust = 1,
           label = t_le0,
           size = 8,
               color = "#f5e042")+
  annotate("text",
           x = Inf,
           y = Inf,
           hjust = 1,
           vjust = 1,
           label = t_g0,
           size = 8,
               color = "#f5e042")
  
```
]

.pull-right[

The **posterior distribution on a $\beta$** value on the left would be interpreted as having **posterior probabilities $p(\beta \le 0) = .4867$ and $p(\beta > 0) = .5133$.** There's no need to **worry about directionality** or anything to **reject**. It's just a **pretty useless $\beta$**. 


]


***


Bayesians often still use **95%** to signify a **reliable difference**. But they don't **freak out** about it: **94%**, for example, **is pretty good, too**. 

Also: posterior probabilities are **legitimate** probabilities, and are not *p*-values.

]

---

### Bayesian Regression with MCMC

Let's compare the **confidence intervals** on ** $\beta_0$** and ** $\beta_1$** estimated by the **frequentist method** (left) and the **Bayesian method** (right)

.pull-left[

```{r}
confint(lm(y ~ x1, data = df))
```


```{r echo = FALSE}
knitr::include_graphics("images/identical-my-cousin-vinny.gif")
```

]

.pull-right[


```{r}
quantile(b0vec, c(0.025, 0.975))
quantile(b1vec, c(0.025, 0.975))
```

***

(well, close enough to identical)

]


---

### Analytic Approach

The other method we used in our **binomial example** was to take advantage of **conjugate prior and posterior distributions**

The **beta distribution** has the **delightful** property of being its **own conjugate function**.

> All we have to do for the beta is update the $\alpha$ and $\beta$ parameters.

It is **no coincidence** that the **estimates** and **confidence intervals** were so close between the frequentist and Bayesian models.

---

### Analytic Approach

In our MCMC example, we used a fairly **generic** set of priors for ** $\beta_0$**, ** $\beta_1$**, and ** $\sigma^2$**.

An **extremely** generic **joint prior distribution**\* we can use for the variables is the distribution ** $\text{NormalGamma}(m_0 = 0, n_0 = 0, s^2_0 = 0, \nu_0 = -1)$**

.slightly-smaller[

> $m_0$ is the prior mean, $n_0$ is the prior sample size, $s^2_0$ is the prior variance, and $\nu_0$ is the prior $df$

> This is a version of the **Jeffreys Prior**, which can be applied to different distributions (*e.g.*  the **Beta distribution**'s Jeffreys Prior is $\alpha=\beta = 1/2$)

]

***

.footnote[
\*technically, it's an *improper distribution* because it doesn't integrate to 1, but the Bayes's theorem math ends up canceling out any problems with that.
]

---

### Analytic Approach

Reminder: in frequentist regression, the coefficients are ** $t$-distributed**.

> *e.g.*, the $p$-values are from $t$-tests.

When we use the **Jeffreys Prior $(\text{NormalGamma}(0, 0, 0, -1))$**, we can show with some **fancy conjugacy math**\* that the **marginal**\*\* distribution of the ** $\beta$** values is **the $t$-distribution**

That means, that our **basic** Bayesian regression is numerically equivalent to frequentist regression (but with much more sensible interpretation).


.footnote[

***
\*A great guide is at https://statswithr.github.io/book/

\*\*Meaning for one variable at a time (as opposed to *joint*)
]

---

### Bayesian Regression

.smaller-output[

```{r message = FALSE}
library(brms)
set.seed(77)
output <- capture.output(fit <- brm(y ~ x1, data = df))
# the output <- capture.output() part is to hide the MCMC procedure
# otherwise, brms likes to tell you EVERYTHING it did.
summary(fit)
```

]

---

### Bayesian Regression

```{r echo = FALSE}
b1_samples <- c(as_draws_array(fit, variable = "b_x1"))
```

.pull-left[

.slightly-smaller[

.smaller-output[

We are **totally good** to say that $\beta_1$ is **reliably different from 0** based on the **credible interval**

```{r}
#all our effects are fixed
#so we use fixef()
round(fixef(fit), 2)
```

]


We could also take the **proportion of samples that are < 0** (see right) as the **posterior probability $p(\beta_1<0)$**

$p(\beta_1 > 0) = `r round(sum(b1_samples < 0)/length(b1_samples), 2)`$


]

]

.pull-right[

.smaller-output[

```{r eval = FALSE}
b1_samples <- c(as_draws_array(fit, variable = "b_x1"))
```

]

```{r echo = FALSE}
data.frame(b1_samples) %>% 
  ggplot(aes(b1_samples))+
  geom_histogram(binwidth = 0.05,
                 color = "#D2E8FF")+
  theme_tufte()+
  theme_xaringan(css_file = "nighttime_in_the_big_city.css")
```


]

---

### Evaluating the Model

We can evaluate the model using **posterior probability** and a **Bayes Factor**, but first we need a **basis for comparison**. We will estimate a **null model**\*:

.smaller-output[

```{r}
output <- capture.output(nullfit <- brm(y ~ 0 + Intercept, data = df))

```

]

.footnote[

***

\**Not* to be confused with a *null hypothesis*; this is a *null model* like we used with **MLMs**
]

---

### Evaluating the Model

The results are the **posterior distribution** on the **mean of the $y$ variable**:

.smaller-output[

```{r}
summary(nullfit)
```

]

---

### Evaluating the Model

We can evaluate the **posterior probability** of the model given the data $(p(\text{model})|D)$ *vs* that of the null model $(p(\text{null model}|D))$

```{r}
post_prob(fit, nullfit)
```


---

### Evaluating the Model

The **Bayes Factor** in favor of the model is given by the ratios of the posterior probabilities:

.smaller-output[

```{r}
bayes_factor(fit, nullfit)
```

]

The evidence for the model is **strong** according to Jeffreys (1961); **very strong** according to Kass & Raftery (1995).

---


### Multiple Regression Example

.smaller-output[

```{r message = FALSE}
library(brms)
set.seed(77)
output <- capture.output(fit2 <- brm(y ~ x1 + x2 + x3, data = df))
summary(fit2)

```

]

---

### Appplying Different Priors

We've seen that basic **noninformative priors** can give us results that mirror **frequentist regression** methods.

We can use **informative priors** to:

> Reflect results of prior research

> Reduce the range of possible results

> Generate more parsimonious models

---

### Applying Different Priors

For example, suppose we ran a **replication study**. We could use the **prior results** to inform the **next investigation**

Here are the **means** and **standard deviations** for our **multiple Bayesian regression example**

```{r}
estimates <- fixef(fit2)[,1]
sds <- fixef(fit2)[,2]
```

```{r}
estimates
```

```{r}
sds
```



---

### Applying Different Priors

We'll use those parameters to define **updated prior distributions** on the **intercept** and **coefficients**

.slightly-smaller[

> note: we're hard-coding the values for *mean* and *sd* because `normal(mean, variance)` is being passed as a character string from `brms` to `Stan`.

]

```{r}
updated_priors <- c(set_prior("normal(9.93, 1.98)",
                          class = "Intercept"),
                    set_prior("normal(-0.45, 0.12)",
                          class = "b",
                          coef = "x1"),
                    set_prior("normal(0.37, 0.14)",
                          class = "b",
                          coef = "x2"),
                    set_prior("normal(0.08, 0.12)",
                          class = "b",
                          coef = "x3"))

```

---

### Applying Different Parameters

.slightly-smaller[

Now, we will conduct the **same analysis** with **updated priors**. For **comparison purposes only**, let's assume that we got **the exact same data** in the replication study

]

.smaller-output[

```{r}
output <- capture.output(fit_inf_priors <- brm(y ~ x1 + x2 + x3, 
                                               data = df,
                                               prior = updated_priors))
summary(fit_inf_priors)
```

]
---

### Applying Different Priors

**Updating the priors** leads to **more precise measurement**

```{r}
fixef(fit2)
```

```{r}
fixef(fit_inf_priors)
```

---

